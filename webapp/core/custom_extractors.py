"""
è‡ªå®šä¹‰è§†é¢‘æå–å™¨
æ”¯æŒç‰¹å®šç½‘ç«™çš„è§†é¢‘æå–ï¼Œå¦‚missav.aiç­‰
"""

import re
import json
import logging
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class MissavExtractor:
    """Missav.ai è§†é¢‘æå–å™¨"""
    
    IE_NAME = 'missav'
    IE_DESC = 'Missav.ai videos'
    
    # æ”¯æŒçš„URLæ¨¡å¼
    _VALID_URL_PATTERNS = [
        r'https?://(?:www\.)?missav\.ai/(?:cn/)?(?P<id>[a-zA-Z0-9\-]+)',
        r'https?://(?:www\.)?missav\.com/(?:cn/)?(?P<id>[a-zA-Z0-9\-]+)',
    ]
    
    def __init__(self):
        self.session = requests.Session()
        # ä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨å¤´éƒ¨æ¥ç»•è¿‡Cloudflare
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
            'Connection': 'keep-alive',
        })

        # æ·»åŠ é‡è¯•æœºåˆ¶
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # ç¦ç”¨ SSL éªŒè¯è­¦å‘Šï¼ˆä»…ç”¨äºç»•è¿‡æŸäº›ç½‘ç«™çš„ SSL é—®é¢˜ï¼‰
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        self.session.timeout = 30
    
    def suitable(self, url):
        """æ£€æŸ¥URLæ˜¯å¦é€‚ç”¨äºæ­¤æå–å™¨"""
        for pattern in self._VALID_URL_PATTERNS:
            if re.match(pattern, url):
                return True
        return False
    
    def extract_info(self, url):
        """æå–è§†é¢‘ä¿¡æ¯"""
        try:
            logger.info(f"ğŸ¬ å¼€å§‹æå– Missav è§†é¢‘: {url}")

            # å°è¯•å¤šç§æ–¹æ³•è·å–ç½‘é¡µå†…å®¹
            response = self._get_webpage_with_retry(url)

            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–åŸºæœ¬ä¿¡æ¯
            title = self._extract_title(soup, url)
            video_id = self._extract_video_id(url)

            # æå–è§†é¢‘æµURL
            video_urls = self._extract_video_urls(soup, response.text, url)

            if not video_urls:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§†é¢‘URLï¼Œå°è¯•ä½¿ç”¨yt-dlpä½œä¸ºå¤‡ç”¨
                logger.warning("è‡ªå®šä¹‰æå–å™¨æœªæ‰¾åˆ°è§†é¢‘æµï¼Œå°è¯•ä½¿ç”¨yt-dlpå¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_to_ytdlp(url)

            # æ„å»ºç»“æœ
            result = {
                'id': video_id,
                'title': title,
                'url': url,
                'formats': video_urls,
                'extractor': 'missav',
                'webpage_url': url,
                'description': self._extract_description(soup),
                'thumbnail': self._extract_thumbnail(soup, url),
                'duration': self._extract_duration(soup),
                'uploader': 'Missav',
                'uploader_id': 'missav',
            }

            logger.info(f"âœ… æˆåŠŸæå–è§†é¢‘ä¿¡æ¯: {title}")
            return result

        except Exception as e:
            logger.error(f"âŒ æå–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨yt-dlpä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨yt-dlpå¤‡ç”¨æ–¹æ¡ˆ")
            try:
                return self._fallback_to_ytdlp(url)
            except Exception as fallback_error:
                logger.error(f"âŒ yt-dlpå¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}")
                raise Exception(f"Missavæå–å¤±è´¥: {str(e)}")

    def _get_webpage_with_retry(self, url):
        """ä½¿ç”¨å¤šç§ç­–ç•¥è·å–ç½‘é¡µå†…å®¹"""
        strategies = [
            self._get_with_chrome_headers,
            self._get_with_firefox_headers,
            self._get_with_mobile_headers,
            self._get_with_edge_headers,
            self._get_with_delay_and_chrome,
            self._get_with_aggressive_bypass,
            self._get_with_standard_headers,
        ]

        last_error = None

        for i, strategy in enumerate(strategies):
            try:
                logger.info(f"ğŸ”„ å°è¯•ç­–ç•¥ {i+1}: {strategy.__name__}")
                response = strategy(url)

                # æ£€æŸ¥æ˜¯å¦è¢«Cloudflareé˜»æ­¢
                if self._is_cloudflare_blocked(response):
                    logger.warning(f"âš ï¸ ç­–ç•¥ {i+1} è¢«Cloudflareé˜»æ­¢")
                    continue

                logger.info(f"âœ… ç­–ç•¥ {i+1} æˆåŠŸ")
                return response

            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ ç­–ç•¥ {i+1} å¤±è´¥: {e}")
                continue

        raise last_error or Exception("æ‰€æœ‰è·å–ç­–ç•¥éƒ½å¤±è´¥")

    def _get_with_standard_headers(self, url):
        """ä½¿ç”¨æ ‡å‡†å¤´éƒ¨è·å–"""
        return self.session.get(url, timeout=30)

    def _get_with_chrome_headers(self, url):
        """ä½¿ç”¨Chromeæµè§ˆå™¨å¤´éƒ¨è·å–"""
        chrome_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        }
        return self.session.get(url, headers=chrome_headers, timeout=30)

    def _get_with_firefox_headers(self, url):
        """ä½¿ç”¨Firefoxæµè§ˆå™¨å¤´éƒ¨è·å–"""
        firefox_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }
        return self.session.get(url, headers=firefox_headers, timeout=30)

    def _get_with_mobile_headers(self, url):
        """ä½¿ç”¨ç§»åŠ¨ç«¯å¤´éƒ¨è·å–"""
        mobile_headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
        }
        return self.session.get(url, headers=mobile_headers, timeout=30)

    def _get_with_edge_headers(self, url):
        """ä½¿ç”¨Edgeæµè§ˆå™¨å¤´éƒ¨è·å–"""
        edge_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Microsoft Edge";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        }
        return self.session.get(url, headers=edge_headers, timeout=30)

    def _get_with_delay_and_chrome(self, url):
        """æ·»åŠ å»¶è¿Ÿåä½¿ç”¨Chromeå¤´éƒ¨è·å–"""
        import time
        time.sleep(3)  # ç­‰å¾…3ç§’

        chrome_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        return self.session.get(url, headers=chrome_headers, timeout=30)

    def _get_with_aggressive_bypass(self, url):
        """ä½¿ç”¨æ¿€è¿›çš„ç»•è¿‡ç­–ç•¥"""
        import time
        import random

        # éšæœºå»¶è¿Ÿ
        time.sleep(random.uniform(2, 5))

        # ä½¿ç”¨æœ€æ–°çš„Chromeå¤´éƒ¨ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
        aggressive_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
            'Connection': 'keep-alive',
            # æ·»åŠ ä¸€äº›é¢å¤–çš„å¤´éƒ¨æ¥æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
            'X-Forwarded-For': f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}',
            'X-Real-IP': f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}',
        }

        # åˆ›å»ºæ–°çš„sessionæ¥é¿å…cookieæ±¡æŸ“
        temp_session = requests.Session()
        temp_session.headers.update(aggressive_headers)

        # è®¾ç½®é‡è¯•æœºåˆ¶
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=2,
            backoff_factor=2,
            status_forcelist=[403, 429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        temp_session.mount("http://", adapter)
        temp_session.mount("https://", adapter)

        try:
            # ç¦ç”¨SSLéªŒè¯ï¼ˆä»…åœ¨å¿…è¦æ—¶ï¼‰
            response = temp_session.get(url, timeout=45, verify=False)
            return response
        finally:
            temp_session.close()

    def _is_cloudflare_blocked(self, response):
        """æ£€æŸ¥æ˜¯å¦è¢«Cloudflareé˜»æ­¢"""
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code in [403, 503, 429]:
            return True

        # æ£€æŸ¥å“åº”å¤´
        cf_headers = ['cf-ray', 'cf-cache-status', 'server']
        for header in cf_headers:
            if header in response.headers:
                header_value = response.headers[header].lower()
                if 'cloudflare' in header_value or 'cf-' in header_value:
                    return True

        # æ£€æŸ¥å“åº”å†…å®¹ä¸­çš„Cloudflareæ ‡è¯†
        content_lower = response.text.lower()
        cloudflare_indicators = [
            'cloudflare',
            'checking your browser',
            'ddos protection',
            'ray id',
            'cf-ray',
            'please wait while we are checking your browser',
            'browser check',
            'security check',
            'just a moment',
            'enable javascript and cookies',
            'attention required',
            'cloudflare to restrict access',
        ]

        # æ£€æŸ¥é¡µé¢é•¿åº¦ï¼ŒCloudflare é˜»æ­¢é¡µé¢é€šå¸¸å¾ˆçŸ­
        if len(response.text) < 1000 and any(indicator in content_lower for indicator in cloudflare_indicators):
            return True

        return any(indicator in content_lower for indicator in cloudflare_indicators)

    def _fallback_to_ytdlp(self, url):
        """ä½¿ç”¨yt-dlpä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        try:
            from .ytdlp_manager import get_ytdlp_manager

            ytdlp_manager = get_ytdlp_manager()

            # å°è¯•å¤šç§yt-dlpé…ç½®
            configs = [
                # é…ç½®1: ä½¿ç”¨impersonationï¼ˆå¦‚æœå¯ç”¨ï¼‰
                {
                    'quiet': True,
                    'no_warnings': True,
                    'extract_flat': False,
                    'skip_download': True,
                    'extractor_args': {
                        'generic': {
                            'impersonate': 'chrome'
                        }
                    }
                },
                # é…ç½®2: ä½¿ç”¨è‡ªå®šä¹‰å¤´éƒ¨
                {
                    'quiet': True,
                    'no_warnings': True,
                    'extract_flat': False,
                    'skip_download': True,
                    'http_headers': {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Accept-Encoding': 'gzip, deflate, br',
                    }
                },
                # é…ç½®3: åŸºç¡€é…ç½®
                {
                    'quiet': True,
                    'no_warnings': True,
                    'extract_flat': False,
                    'skip_download': True,
                }
            ]

            last_error = None

            for i, ydl_opts in enumerate(configs):
                try:
                    logger.info(f"ğŸ”„ å°è¯•yt-dlpé…ç½® {i+1}")

                    with ytdlp_manager.create_downloader(ydl_opts) as ydl:
                        info = ydl.extract_info(url, download=False)

                        if info:
                            logger.info(f"âœ… yt-dlpé…ç½® {i+1} æˆåŠŸ")
                            return info

                except Exception as e:
                    last_error = e
                    logger.warning(f"âš ï¸ yt-dlpé…ç½® {i+1} å¤±è´¥: {e}")
                    continue

            raise last_error or Exception("æ‰€æœ‰yt-dlpé…ç½®éƒ½å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ yt-dlpå¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
            # å¦‚æœyt-dlpä¹Ÿå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºç¡€çš„ä¿¡æ¯ç»“æ„
            return self._create_fallback_info(url)

    def _create_fallback_info(self, url):
        """åˆ›å»ºå¤‡ç”¨ä¿¡æ¯ç»“æ„"""
        video_id = self._extract_video_id(url)

        # åˆ›å»ºä¸€ä¸ªåŸºç¡€çš„ä¿¡æ¯ç»“æ„ï¼Œè®©ç”¨æˆ·çŸ¥é“éœ€è¦æ‰‹åŠ¨å¤„ç†
        return {
            'id': video_id,
            'title': f"Missav_{video_id}",
            'url': url,
            'formats': [],  # ç©ºæ ¼å¼åˆ—è¡¨ï¼Œè¡¨ç¤ºéœ€è¦æ‰‹åŠ¨å¤„ç†
            'extractor': 'missav_fallback',
            'webpage_url': url,
            'description': 'ç”±äºç½‘ç«™ä¿æŠ¤æœºåˆ¶ï¼Œæ— æ³•è‡ªåŠ¨æå–è§†é¢‘ä¿¡æ¯ã€‚è¯·å°è¯•æ‰‹åŠ¨ä¸‹è½½æˆ–ç¨åé‡è¯•ã€‚',
            'thumbnail': None,
            'duration': None,
            'uploader': 'Missav',
            'uploader_id': 'missav',
            '_fallback': True,  # æ ‡è®°ä¸ºå¤‡ç”¨ä¿¡æ¯
        }

    def _extract_title(self, soup, url):
        """æå–è§†é¢‘æ ‡é¢˜"""
        # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
        title_selectors = [
            'h1.text-secondary',
            'h1',
            '.video-title',
            'title',
            'meta[property="og:title"]',
            'meta[name="title"]'
        ]
        
        for selector in title_selectors:
            try:
                if selector.startswith('meta'):
                    element = soup.select_one(selector)
                    if element:
                        title = element.get('content', '').strip()
                        if title:
                            return self._clean_title(title)
                else:
                    element = soup.select_one(selector)
                    if element:
                        title = element.get_text().strip()
                        if title:
                            return self._clean_title(title)
            except Exception as e:
                logger.debug(f"æ ‡é¢˜æå–å¤±è´¥ ({selector}): {e}")
                continue
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä»URLæå–
        video_id = self._extract_video_id(url)
        return f"Missav_{video_id}"
    
    def _clean_title(self, title):
        """æ¸…ç†æ ‡é¢˜"""
        # ç§»é™¤ç½‘ç«™åç§°
        title = re.sub(r'\s*-\s*MISSAV.*$', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\s*\|\s*MISSAV.*$', '', title, flags=re.IGNORECASE)
        
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        title = re.sub(r'[<>:"/\\|?*]', '_', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        return title or "Missav_Video"
    
    def _extract_video_id(self, url):
        """ä»URLæå–è§†é¢‘ID"""
        for pattern in self._VALID_URL_PATTERNS:
            match = re.match(pattern, url)
            if match:
                return match.group('id')
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä»URLè·¯å¾„æå–
        path = urlparse(url).path
        video_id = path.split('/')[-1]
        return video_id or 'unknown'
    
    def _extract_video_urls(self, soup, html_content, page_url):
        """æå–è§†é¢‘æµURL"""
        video_urls = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾JavaScriptä¸­çš„æ’­æ”¾åˆ—è¡¨URL
        m3u8_patterns = [
            r'["\']([^"\']*\.m3u8[^"\']*)["\']',
            r'playlist\s*:\s*["\']([^"\']+)["\']',
            r'src\s*:\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
            r'file\s*:\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
        ]
        
        for pattern in m3u8_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if '.m3u8' in match:
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    if match.startswith('http'):
                        video_url = match
                    else:
                        video_url = urljoin(page_url, match)
                    
                    video_urls.append({
                        'url': video_url,
                        'format_id': 'hls',
                        'ext': 'mp4',
                        'protocol': 'm3u8',
                        'quality': 'unknown'
                    })
                    logger.info(f"ğŸ¯ æ‰¾åˆ°è§†é¢‘æµ: {video_url}")
        
        # æ–¹æ³•2: æŸ¥æ‰¾videoæ ‡ç­¾
        video_tags = soup.find_all('video')
        for video in video_tags:
            src = video.get('src')
            if src and ('.m3u8' in src or '.mp4' in src):
                if not src.startswith('http'):
                    src = urljoin(page_url, src)
                
                video_urls.append({
                    'url': src,
                    'format_id': 'direct',
                    'ext': 'mp4',
                    'quality': 'unknown'
                })
        
        # æ–¹æ³•3: æŸ¥æ‰¾sourceæ ‡ç­¾
        source_tags = soup.find_all('source')
        for source in source_tags:
            src = source.get('src')
            if src and ('.m3u8' in src or '.mp4' in src):
                if not src.startswith('http'):
                    src = urljoin(page_url, src)
                
                video_urls.append({
                    'url': src,
                    'format_id': 'source',
                    'ext': 'mp4',
                    'quality': 'unknown'
                })
        
        # å»é‡
        seen_urls = set()
        unique_urls = []
        for video_url in video_urls:
            if video_url['url'] not in seen_urls:
                seen_urls.add(video_url['url'])
                unique_urls.append(video_url)
        
        return unique_urls
    
    def _extract_description(self, soup):
        """æå–è§†é¢‘æè¿°"""
        desc_selectors = [
            'meta[property="og:description"]',
            'meta[name="description"]',
            '.video-description',
            '.description'
        ]
        
        for selector in desc_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith('meta'):
                        desc = element.get('content', '').strip()
                    else:
                        desc = element.get_text().strip()
                    
                    if desc:
                        return desc
            except:
                continue
        
        return None
    
    def _extract_thumbnail(self, soup, page_url):
        """æå–ç¼©ç•¥å›¾"""
        thumb_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'video[poster]',
            '.video-thumbnail img',
            '.thumbnail img'
        ]
        
        for selector in thumb_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith('meta'):
                        thumb = element.get('content', '').strip()
                    elif 'poster' in selector:
                        thumb = element.get('poster', '').strip()
                    else:
                        thumb = element.get('src', '').strip()
                    
                    if thumb:
                        if not thumb.startswith('http'):
                            thumb = urljoin(page_url, thumb)
                        return thumb
            except:
                continue
        
        return None
    
    def _extract_duration(self, soup):
        """æå–è§†é¢‘æ—¶é•¿"""
        duration_selectors = [
            'meta[property="video:duration"]',
            '.duration',
            '.video-duration'
        ]
        
        for selector in duration_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith('meta'):
                        duration_str = element.get('content', '').strip()
                    else:
                        duration_str = element.get_text().strip()
                    
                    if duration_str:
                        # å°è¯•è§£ææ—¶é•¿
                        return self._parse_duration(duration_str)
            except:
                continue
        
        return None
    
    def _parse_duration(self, duration_str):
        """è§£ææ—¶é•¿å­—ç¬¦ä¸²"""
        try:
            # æ ¼å¼: "MM:SS" æˆ– "HH:MM:SS"
            if ':' in duration_str:
                parts = duration_str.split(':')
                if len(parts) == 2:  # MM:SS
                    return int(parts[0]) * 60 + int(parts[1])
                elif len(parts) == 3:  # HH:MM:SS
                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
            
            # æ ¼å¼: çº¯æ•°å­—ï¼ˆç§’ï¼‰
            return int(duration_str)
        except:
            return None


# å…¨å±€æå–å™¨å®ä¾‹
missav_extractor = MissavExtractor()


def get_custom_extractor(url):
    """æ ¹æ®URLè·å–åˆé€‚çš„è‡ªå®šä¹‰æå–å™¨"""
    if missav_extractor.suitable(url):
        return missav_extractor
    
    return None


def extract_with_custom_extractor(url):
    """ä½¿ç”¨è‡ªå®šä¹‰æå–å™¨æå–è§†é¢‘ä¿¡æ¯"""
    extractor = get_custom_extractor(url)
    if extractor:
        return extractor.extract_info(url)
    
    raise Exception(f"æ²¡æœ‰æ‰¾åˆ°é€‚åˆçš„è‡ªå®šä¹‰æå–å™¨: {url}")
